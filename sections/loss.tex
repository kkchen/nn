\section{Loss}

\subsection{}

\begin{frame}
    \frametitle{How good is a prediction?}

    \begin{itemize}
        \item $\x \in \X$: data \& model input
        \item $\y \in \Y$: data output
        \item $\yh \in \Yh$: model output with $\dim(\Yh) = \dim(\Y)$ (generally, $\Yh = \Y$)
        \item $\f : \X \to \Yh, \x \mapsto \yh$: model
    \end{itemize}
    \pause

    \begin{block}{Loss function}
        A function that specifies how bad of a prediction $\yh = \f(\x)$ is for the ground truth $\y$
        \begin{equation*}
            L : \Y \times \Yh \to \Reals_{\ge 0}
        \end{equation*}
    \end{block}
    \pause

    We will see that ``training a machine learning algorithm'' basically means ``minimizing the expected loss over the training data.'' \\[1ex]

    You get to pick the loss function!
\end{frame}

\begin{frame}
    \frametitle{Regression loss functions}
    Let $\y = \begin{bmatrix} y_1 & \cdots & y_n \end{bmatrix}$,
    $\yh = \begin{bmatrix} \hat{y}_1 & \cdots & \hat{y}_n \end{bmatrix}$

    \begin{block}{Mean squared error}
        \vspace{-1em}
        \begin{align*}
            L(\y, \yh) &:= \frac{1}{n} \sum_{i=1}^n (y_n - \hat{y}_n)^2 \\
            &= \dfrac{\|\y - \yh\|_2^2}{n}
        \end{align*}
        \vspace{-1em}
        \begin{itemize}
            \item Most common loss for regression
        \end{itemize}
    \end{block}
    \pause

    \begin{block}{Mean absolute error}
        \vspace{-1em}
        \begin{align*}
            L(\y, \yh) &:= \frac{1}{n} \sum_{i=1}^n |y_n - \hat{y}_n| \\
            &= \dfrac{\|\y - \yh\|_1}{n}
        \end{align*}
        \vspace{-1em}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Classification loss functions}
    Assumptions:
    \begin{itemize}
        \item<+-> correct label $= k$; $\y = \e_k$ (one-hot)
        \item<.-> $\yh =
        \begin{bmatrix}
            \hat{y}_1 & \cdots & \hat{y}_n
        \end{bmatrix} =
        \begin{bmatrix}
            \Prob(\text{label} = 1) & \cdots & \Prob(\text{label} = n)
        \end{bmatrix}$
    \end{itemize}

    \begin{block}{Zero--one loss}<+->
        \begin{equation*}
            L(\y, \yh) := \begin{cases}
                0 &\mid \argmax(\yh) = k \\
                1 &\mid \argmax(\yh) \ne k
            \end{cases}
        \end{equation*}
        \begin{itemize}
            \item Easiest, but non-differentiable
        \end{itemize}
    \end{block}

    \begin{block}{(Categorical) cross-entropy loss}<+->
        \begin{equation*}
            L(\y, \yh) := -\log\hat{y}_k = \log\left(\dfrac{1}{\Prob(\text{label} = k)}\right)
        \end{equation*}
        \begin{itemize}
            \item Most common loss for labeling with $> 2$ classes
            \item $L = 0 \iff P(\text{label} = k) = 1$;
            $L \to \infty \iff P(\text{label} = k) \to 0$
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Classification loss function: more cross-entropy}
    Recall: for binary problems, classes are $y = 0$ (no) and $1$ (yes)

    \begin{block}{Binary cross-entropy loss}
        \vspace{-1em}
        \begin{align*}
            L(y, \hat{y}) &:= -y \log \hat{y} - (1 - y) \log(1 - \hat{y}) \\
            &= \begin{cases}
                -\log(1 - \hat{y}) &\mid y = 0 \\
                -\log \hat{y} &\mid y = 1
            \end{cases}
        \end{align*}
    \end{block}
    \pause

    Note: high accuracy $\centernot\implies$ small loss
    \begin{itemize}
        \item Typically, model prediction taken to be $\argmax(\yh)$
        \item Cross-entropy penalizes being unconfident about correct label, \emph{not} having the wrong $\argmax(\yh)$
        \pause
        \item E.g., if $k = 3$ is correct answer, cross-entropy:
        \begin{align*}
            L(
                \begin{bmatrix} 0 & 0 & 1 & 0 \end{bmatrix},
                \begin{bmatrix}
                    0.05 & 0.03 & \textcolor{blue}{0.9} & 0.02
                \end{bmatrix}
            ) &= 0.105 \\
            L(
                \begin{bmatrix} 0 & 0 & 1 & 0 \end{bmatrix},
                \begin{bmatrix}
                    0.25 & 0.25 & \textcolor{blue}{0.26} & 0.24
                \end{bmatrix}
            ) &= 1.35
        \end{align*}
        even though both $\yh$ correctly predict $\argmax(\yh) = 3$
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Model risk and batch loss}

    The \alert{model risk} is the expected loss over all $\x \in \X$ with distribution $p_\X$:

    \begin{block}{Model risk}
        \vspace{-1em}
        \begin{align*}
            R(\f) &:= \E_{\x \sim p_\X}(\E_{\y \mid \x}(L(\y, \f(\x)))) \\
            &= \int_\X \int_\Y L(\y, \f(\x)) \Prob(\y | \x) \, \dy \Prob(\x) \, \dx
        \end{align*}
    \end{block}
    \pause

    Since we usually can't exhaust over all $\x \in \X$, select a batch $\S = \{(\x_i, \y_i)\}_{i=1}^m \subset \X \times \Y$, then use:

    \begin{block}{Approximate model risk / batch loss}
        \vspace{-1em}
        \begin{align*}
            \hat{R}(\f) &:= \E_{(\x, \y) \in \S}(L(\y, \f(\x))) \\
            &= \frac{1}{m} \sum_{i=1}^m L(\y_i, \f(\x_i))
        \end{align*}
    \end{block}
    \pause

    Objective of ML training: \alert{minimize $\hat{R}$ over some data set}
    \begin{itemize}
        \item From now on I'll just call it $R$
    \end{itemize}
\end{frame}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../nn"
%%% End:
