\section{Regularization}

\subsection{}

\begin{frame}
    \frametitle{Underfitting and overfitting}

    \begin{alertblock}{Danger!}
        Neural networks are hard to fit, and real-life data are noisy.
        \emph{Underfitting} and \emph{overfitting} are your two biggest obstacles.
    \end{alertblock}
    \vspace{1ex}

    \includegraphics{under_over_train}
    \pause

    \begin{itemize}
        \item Underfitting: didn't train enough for the model to capture the underlying behavior
        \item Overfitting: training so good that neural network memorizes the data, including underlying noise
        \begin{itemize}
            \item Recall: with enough parameters, neural networks learn anything
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Train, validate, test}

    Usual trick: split data into disjoint \textcolor{blue}{training} and \textcolor{Green4}{testing} sets
    \begin{itemize}
        \item Anywhere from 95/5 to 75/25 split is common
        \item Train \emph{only} on the \textcolor{blue}{training set}
        \item Periodically check loss of NN on \textcolor{Green4}{test set}
        \begin{itemize}
            \item Optimizer never sees test set: test set is honest check on NN fidelity
        \end{itemize}
    \end{itemize}
    \pause

    More advanced: split into disjoint \textcolor{blue}{training}, \textcolor{orange}{validation}, and \textcolor{Green4}{testing} set
    \begin{itemize}
        \item Aside on terminology
        \begin{itemize}
            \item Weights \& biases are \alert{parameters}---iteratively improved
            \item Design choices (\# layers, \# neurons, optimizer, \# epochs, etc.) are \alert{hyperparameters}---selected by ML designer
        \end{itemize}
        \item Select different sets of hyperparameters; for each:
        \begin{itemize}
            \item Train only on \textcolor{blue}{training set}, as before
            \item Compute validation loss on \textcolor{orange}{validation set}
        \end{itemize}
        \item Pick hyperparameters with lowest validation loss
        \item Then re-evaluate loss on \textcolor{Green4}{test data}
        \begin{itemize}
            \item Helps remove effects of variance on validation loss
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Training loss vs.\ test loss}
    \includegraphics{loss}

    General pattern
    \begin{itemize}
        \item Modulo minor stochastic fluctuations, training loss should monotonically decrease: NN gets better at learning data
        \item Test loss usually at least slightly larger than training loss
        \begin{itemize}
            \item Because optimizer minimizes training loss but never sees test data
        \end{itemize}
        \item At some point, test loss increases: NN overfits to training data
        \item \alert{Early stopping}: use NN with minimum test loss
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Some regularization techniques (1/2)}

    \begin{block}{}
        Regularization: modifications that reduce generalization error
    \end{block}

    \begin{itemize}
        \item<+-> Early stopping
        \item<+-> Don't make NNs too big
        \begin{itemize}
            \item Rule of thumb: $\text{\# parameters} \le \text{\# data}$
        \end{itemize}
        \item<+-> $L^2$ regularization: use loss $\hat{L}(\y, \f(\x; \THETA)) = L(\y, \f(\x; \THETA)) + \alpha \|\THETA\|_2^2$
        \begin{itemize}
            \item Reduces weights, especially in directions that affect $L$ little
        \end{itemize}
        \item<+-> $L^1$ regularization: use loss $\hat{L}(\y, \f(\x; \THETA)) = L(\y, \f(\x; \THETA)) + \alpha \|\THETA\|_1$
        \begin{itemize}
            \item Promotes sparsity in $\THETA$
        \end{itemize}
        \item<+-> Add noise to NN input during training \citep{SietsmaNN91}
        \begin{itemize}
            \item Very non-intuitive to engineers
            \item Prevents overfitting: ``fuzzes'' out bad data
        \end{itemize}
        \item<+-> Bagging (bootstrap aggregating) \citep{BreimanML94}
        \begin{itemize}
            \item For data of size $n$, draw $d$ samples with replacement $m$ times
            \item Train one NN on each of the $m$ sets \& average results
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Some regularization techniques (2/2)}
    \begin{itemize}
        \item<+-> \alert{Dropout} \citep{SrivastavaJMLR14}
        \begin{itemize}[<.->]
            \item During \textcolor{Green4}{training}, randomly remove some portion of connections
            \item Often 20\% of inputs and 50\% of hidden connections
            \item<+-> Put all connections back \& scale accordingly during \textcolor{blue}{inference}
            \item<+-> Motivation: kind of like bagging, but much cheaper
            \item With $n \gg 1$ iterations, essentially average of $n$ models
            \item One of the best regularization techniques; use it!
        \end{itemize}
    \end{itemize}

    \input{figures/dropout1}
    \hfill
    \input{figures/dropout2}
    \hfill
    \input{figures/dropout3}
    \hfill
    \uncover<2->{\input{figures/no_dropout}}

    \begin{itemize}
        \item \alert{Batch normalization} \citep{IoffeICML15}
        \begin{itemize}
            \item Automatically shifts \& scales values to have mean 0 \& variance 1 for each mini-batch
            \item Results in better behavior and easier learning: inputs to activations won't fly to $\pm \infty$
        \end{itemize}
    \end{itemize}
\end{frame}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../nn"
%%% End:
