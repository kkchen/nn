\section[CNNs]{Convolutional neural networks}

\subsection{}

\begin{frame}
    \frametitle{Motivation}

    \begin{block}{Disclaimer}
        I know almost nothing about CNNs, but not discussing them would be sacrilegious.
        I'll do my best.
    \end{block}

    \begin{itemize}
        \item Much of deep learning revolves around image classification
        \item A huge proportion of early major breakthroughs in deep learning involved CNNs for image classification
    \end{itemize}

    \begin{block}{Spatial invariance}
        CNNs leverage the fact that images are \alert{spatially invariant}
        \begin{itemize}
            \item A dog is a dog whether its head is in the left or right side of an image
            \item Dense networks treat every input as independent
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{2-D convolution}
    \begin{columns}
        \begin{column}{0.41\textwidth}
            \input{figures/convolution}
        \end{column}
        \begin{column}{0.59\textwidth}
            Given
            \begin{itemize}[<+->]
                \item \textcolor{blue}{RGB image}, dimension $w \times h \times 3$
                \item A \alert{filter}/\alert{kernel}, dimension $m \times m \times 3$, $m = \O(1 \text{--} 10) \ll w, h$
            \end{itemize}
            \uncover<+->{
                For every $m \times m$ block in the 2-D image, \textcolor{Green4}{convolve} with the filter:
                }
            \begin{itemize}
                \item<.-> Element-wise multiply $m \times m \times 3$ block by filter ($\Reals^{m \times m \times 3} \to \Reals^{m \times m \times 3}$)
                \item<.-> Sum the result ($\Reals^{m \times m \times 3} \to \Reals$)
                \item<9-> $w - m + 1$ horizontal block positions \& $h - m + 1$ vertical block positions $\implies$ produces \textcolor{Green4}{feature map} in $\Reals^{(w-m+1) \times (h-m+1)}$
            \end{itemize}
            \uncover<10->{Repeat for $k$ filters}
            \begin{itemize}
                \item<10-> Yields $k$ feature maps in $\Reals^{(w-m+1) \times (h-m+1) \times k}$
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{What's the point?}

    \begin{columns}
        \begin{column}{0.31\textwidth}
            \input{figures/filter}
        \end{column}
        \begin{column}{0.69\textwidth}
            Overly simplified example:
            \begin{itemize}[<+->]
                \item Suppose one of the class label is ``car''
                \item Cars have wheels
                \item Suppose one of the filters has a round shape
                \item For most $m \times m \times 3$ blocks, convolution produces noise
                \item<10-> But if block contains tire, then convolution hits $\implies$ large value
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Filters \& pooling}

    \begin{columns}
        \begin{column}{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{filters} \\
            \footnotesize
            AlexNet filters \citep{KrizhevskyNIPS12}
        \end{column}

        \begin{column}{0.6\textwidth}
            Reality not that simple
            \begin{itemize}
                \item Trained filters find edges, arcs, color patches, etc.
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Rich history that authors expect you to know}
    \begin{columns}
        \begin{column}{0.55\textwidth}
            \begin{itemize}
                \item<+-> Late 1990s: MNIST database of handwritten digits
                \begin{itemize}
                    \item 60,000 training samples, 10,000 test samples
                    \item $28 \times 28$ pixels, black and white
                    \item CNN test error: 0.7\% \citep[``LeNet-5'',][]{LeCunIEEE98} to 0.21\% \citep{WanICML13}
                \end{itemize}
                \item<+-> Early 2010s: CIFAR-10/100 database, 10/100 image labels
                \begin{itemize}
                    \item 50,000 training samples, 10,000 test samples
                    \item $32 \times 32$ color images
                    \item CIFAR-10 test error: 35\% \citep{RanzatoAISTATS10} to 2.1\% \citep{Real18}
                \end{itemize}
                \item<+-> Early 2010s: ImageNet---$\O(10^7)$ images, $\O(10^4)$ labels
            \end{itemize}
        \end{column}
        \begin{column}{0.45\textwidth}
            \includegraphics[width=\textwidth]{mnist} \\[5mm]
            \uncover<.->{\includegraphics[width=\textwidth]{imagenet}}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{CNN architectures that authors expect you to know}

    \% ImageNet test samples with labels not in top 5:
    \begin{itemize}
        \item<+-> LeNet \citep{LeCunIEEE98}: 18.9\% top-5 error
        \begin{itemize}
            \item 2 convolutional layers (size $5 \times 5 \times 6$, $5 \times 5 \times 6 \times 16$) $\to$ 3 dense layers (size 120, 84, 10)
        \end{itemize}
        \item<+-> AlexNet \citep{KrizhevskyNIPS12}: 17.0\%
        \begin{itemize}
            \item 5 convolutional layers (size $11 \times 11 \times 3 \times 96$, $5 \times 5 \times 48 \times 256$, $3 \times 3 \times 256 \times 384$, $3 \times 3 \times 192 \times 384$, $3 \times 3 \times 192 \times 256$) $\to$ 3 dense layers (size 4096, 4096, 1000)
        \end{itemize}
        \item<+-> ZF Net \citep{ZeilerECCV14}: 11.2\%
        \begin{itemize}
            \item Minor improvement to AlexNet
        \end{itemize}
        \item<+-> VGGNet \citep{Simonyan14}: 7.7\%
        \begin{itemize}
            \item Replace AlexNet's $11 \times 11$ and $5 \times 5$ filters with deep $3 \times 3$ filters
        \end{itemize}
        \item<+-> GoogLeNet/Inception \citep{SzegedyIEEECVPR15}: 6.67\%
        \begin{itemize}
            \item Reduces work by using \emph{Inception modules} that combine size 1, 3, and 5 convolutions
        \end{itemize}
        \item<+-> ResNet \citep{He15b}: 3.57\%
        \begin{itemize}
            \item Instead of feeding convolutional output to next layer, feed convolutional input + output
        \end{itemize}
    \end{itemize}
\end{frame}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../nn"
%%% End:
