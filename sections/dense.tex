\section{Dense neural networks}

\subsection{}

\begin{frame}
    \frametitle{The neuron: equations}

    Consider an input $\x \in \Reals^q$.
    \begin{block}{}
        Consider some \emph{weight vector} $\w \in \Reals^q$ and \emph{bias} $b \in \Reals$.
        The mapping
        \begin{align*}
            g &: \Reals^q \to \Reals \\
            &\hspace{1.25ex} \x \mapsto \w \cdot \x + b
        \end{align*}
        is an \alert{affine transformation}.
    \end{block}
    \pause

    Consider a generally nonlinear \alert{activation function} $\sigma: \Reals \to \Reals$.

    \begin{block}{}
        An (artifical) \alert{neuron} is the function $\phi = \sigma \circ g$, i.e.,
        \begin{align*}
            \phi &: \Reals^q \to \Reals \\
            &\hspace{1.25ex} \x \mapsto \sigma(\w \cdot \x + b)
        \end{align*}
    \end{block}
    \pause

    \begin{itemize}
        \item Historically, artificial neuron modeled on biological neurons
        \begin{itemize}
            \item Bio neuron triggers impulse by nonlinear function of inputs
        \end{itemize}
        \item Today, relation is largely only conceptual/philosophical
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{The neuron: geometric interpretation}

    \begin{itemize}
        \item Activation functions $\sigma(x)$ typically most ``interesting'' at $x = 0$
        \item For neuron $\phi(x) = \sigma(\w \cdot \x + b)$, \textcolor{Green4}{hyperplane} $\w \cdot \x + b = 0$ is a natural boundary in $\X$
        \item Simple example:
        \begin{itemize}
            \item $\x =$ (sex, age, weight, height, LDL, HDL, exercise hrs/wk, smoker)
            \item $y =$ heart disease risk
            \item Medical intuition: larger risk for men, older, heavier, shorter, higher LDL, lower HDL, less exercise, smoker
            \item Geometric interpretation: draw \textcolor{Green4}{hyperplane} through $\Reals^8$ best separating \alert{high-risk} from \textcolor{blue}{low-risk}
        \end{itemize}
    \end{itemize}

    \centering
    \includegraphics{plane}
\end{frame}

\begin{frame}
    \frametitle{Single hidden-layer neural network: vector equations}

    Consider $n$ neurons each taking in $\x \in \Reals^q$: given
    \begin{itemize}
        \item weight vectors $\w_k \in \Reals^q$, $k = 1, \ldots, n$,
        \item biases $b_k \in \Reals$, $k = 1, \ldots, n$,
    \end{itemize}
    \begin{block}{Hidden layer}
        \vspace{-1em}
        \begin{align*}
            z_k &= \sigma(\w_k \cdot \x + b_k), \quad k = 1, \ldots, n \\
            \z &= \begin{bmatrix} z_1 & \cdots & z_n \end{bmatrix} \in \Reals^n
        \end{align*}
    \end{block}
    \pause

    Let the model output $\y \in \Reals^p$ be affine transformations of $\z$: given
    \begin{itemize}
        \item weight vectors $\v_k \in \Reals^n$, $k = 1, \ldots, p$
        \item biases $c_k \in \Reals$, $k = 1, \ldots, p$
    \end{itemize}
    \begin{block}{Single hidden-layer dense neural network}
        \vspace{-1em}
        \begin{align*}
            y_k &= \v_k \cdot \z + c_k, \quad k = 1, \ldots, p \\
            \y &= \begin{bmatrix} y_1 & \cdots & y_p \end{bmatrix}
        \end{align*}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Single hidden-layer neural network: figure}

    {
        \centering
        \input{figures/shallow}
    }
    \vspace{0.5ex}

    Dense neural network, a.k.a.~feedforward neural network, a.k.a.~multi-layer perceptron
\end{frame}

\begin{frame}
    \frametitle{Single hidden-layer neural network: matrix equations}

    The equations are more compact in matrix form:
    \begin{itemize}
        \item Weight matrix $\W \in \Reals^{n \times q}$
        \item Bias vector $\b \in \Reals^n$
        \item Element-wise activation function
        \begin{align*}
            \SIGMA &: \Reals^n \to \Reals^n \\
            &\hspace{1.25ex} \begin{bmatrix} \xi_1 & \cdots & \xi_n \end{bmatrix} \mapsto
            \begin{bmatrix} \sigma(\xi_1) & \cdots & \sigma(\xi_n) \end{bmatrix}
        \end{align*}
    \end{itemize}

    \begin{block}{Hidden layer}
        \begin{equation*}
            \z = \SIGMA(\W \x + \b)
        \end{equation*}
    \end{block}
    \pause

    \begin{itemize}
        \item Weight matrix $\V \in \Reals^{q \times n}$
        \item Bias vector $\cc \in \Reals^q$
    \end{itemize}

    \begin{block}{Single hidden-layer dense neural network}
        \begin{equation*}
            \y = \V \z + \cc
        \end{equation*}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Activation functions}
    \begin{block}{Why do neurons have nonlinear activation functions?}
        If $\sigma$ is linear (including the identity $\sigma(x) = x$, i.e. no activation), the neural network $\f : \x \mapsto \y$ is necessarily linear \\[1ex]
        For $\f$ to be nonlinear, \alert{activation functions must be nonlinear}!
    \end{block}

    In some sense, it doesn't even matter what $\sigma$ is, as long as it's nonlinear. \\[1ex]

    To be more particular, $\sigma$ should
    \begin{itemize}
        \item be fast to compute (will be called gazillions of times)
        \item not lead to vanishing or exploding gradients
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Common activation functions}
    \begin{columns}
        \begin{column}{2.5in}
            \begin{itemize}
                \item \textcolor{Green4}{Standard logistic/sigmoid function: $\sigma(x) = \dfrac{1}{1 + e^{-x}} = \dfrac{\tanh(x) + 1}{2}$}
                \begin{itemize}
                    \item Used to be preferred
                    \item Now thought to be too slow
                    \item Suffers from vanishing gradients
                \end{itemize}
                \item \textcolor{blue}{Rectified linear unit (ReLU): $\sigma(x) = \max(0, x)$}
                \begin{itemize}
                    \item Most common, but suffers from vanishing gradients
                \end{itemize}
            \end{itemize}
        \end{column}
        \begin{column}{2in}
            \includegraphics{activations}
        \end{column}
    \end{columns}

    % Ugly hack to make the bullets line up.
    \begin{columns}
        \begin{column}{4.7in}
            \begin{itemize}
                \item \textcolor{red}{Leaky/parametric ReLU} \citep{MaasICML13,He15a}:
                \textcolor{red}{
                    $\sigma(x) = \begin{cases}
                        x &\mid x \ge 0 \\
                        \alpha x &| x < 0
                    \end{cases}$
                }
                \item \textcolor{Magenta3}{Exponential linear unit (ELU)} \citep{ClevertICLR16}:
                \textcolor{Magenta3}{
                    $\sigma(x) = \begin{cases}
                        x &\mid x > 0 \\
                        \alpha (e^x - 1) &\mid x \le 0
                    \end{cases}$
                }
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Deep neural networks}

    No reason to stop at only one hidden layer!

    \vspace{1mm}
    \input{figures/deep}
    \vspace{1mm}

    Common but not necessary for $\SIGMA$ to be the same in each layer
\end{frame}

\begin{frame}
    \frametitle{Why is deep better than wide?}
    \begin{itemize}
        \item Depth: \# layers
        \item Width: \# neurons in each layer (need not be fixed)
    \end{itemize}
    \pause

    Neural network with $\sigma = \text{ReLU}$ is piecewise linear function
    \begin{itemize}
        \item \alert{Expressivity} of ReLU network measured by \# linear pieces
        \item For fixed width $w$ and depth $l$, \alert{$\max \text{expressivity} \sim w^l$} \citep{Pascanu13,MontufarNIPS14,Chen16}
        \item In comparison, $\alert{\text{\# parameters}} = (q + 1) n_1 + (n_1 + 1) n_2 + \cdots + (n_{l-1} + 1) n_l + (n_l + 1) p \approx \alert{w^2 l}$
    \end{itemize}
    \pause

    Common for layers closer to the input to be wider
    \begin{itemize}
        \item Closer to the input $\implies$ more expressive power over output \citep{RaghuICML17}
    \end{itemize}
    \pause

    \begin{block}{}
        Depth is (arguably) the key reason for the ML/AI explosion in the last decade.
        Recall ResNet---152 layers deep!
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{%
        Example: $\Reals^2 \to \Reals$ ripple\footnote{%
            Code: \blueurl{https://goo.gl/Yv8CW3}.
            Trained to convergence: $\sim8 \cdot 10^7$ samples.%
        }%
    }
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \centering
            \includegraphics[width=1.85in]{ripple} \\
            Ground truth: $y = \dfrac{\cos(r)}{e^{0.4 (r - 8)} + 1}$
        \end{column}
        \hfill
        \pause
        \begin{column}{0.5\textwidth}
            \centering
            \includegraphics[width=1.85in]{ripple_1layer} \\
            1 layer, 50 neurons, \\ 201 parameters: $R = 8.1 \cdot 10^{-2}$
        \end{column}
    \end{columns}
    \pause

    \begin{columns}
        \begin{column}{0.5\textwidth}
            \centering
            \includegraphics[width=1.85in]{ripple_2layer} \\
            2 layers, 17--8 neurons, \\ 204 parameters: $R = 4.9 \cdot 10^{-3}$
        \end{column}
        \hfill
        \pause
        \begin{column}{0.5\textwidth}
            \centering
            \includegraphics[width=1.85in]{ripple_3layer} \\
            3 layers, 15--7--4 neurons, \\ 194 parameters: $R = 1.6 \cdot 10^{-3}$
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{What can dense neural networks model?}
    \begin{block}{Universal approximation theorem}
        Anything, basically, if $\text{width} \to \infty$
        \begin{itemize}
            \item Depth not required; 1 hidden layer is enough
        \end{itemize}
    \end{block}

    Some mild conditions required.
    Limitations on $\sigma$:
    \begin{itemize}
        \item $\sigma$ is sigmoid \citep{CybenkoMCSS89,HornikNN89,HornikNN91}
        \item $\sigma$ can be ReLU, polynomial, Gaussian, etc. \citep{SonodaACHA}
    \end{itemize}
\end{frame}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../nn"
%%% End:
