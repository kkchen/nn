@misc{Chen16,
    author = {Chen, Kevin K.},
    title = {The Upper Bound on Knots in Neural Networks},
    year = 2016,
    month = nov,
    howpublished = {arXiv:1611.09448}
}

@inproceedings{ClevertICLR16,
    author = {Clevert, Djork-Arn\'e and Unterthiner, Thomas and Hochreiter, Sepp},
    title = {Fast and Accurate Deep Network Learning by {Exponential Linear Units (ELUs)}},
    booktitle = {Sixth International Conference on Learning Representations ({ICLR})},
    year = 2016,
    month = may,
    address = {San Juan, Puerto Rico}
}

@article{CybenkoMCSS89,
    author = {Cybenko, George},
    title = {Approximation by superpositions of a sigmoidal function},
    journal = {Math. Control Signals Syst.},
    volume = {2},
    number = {4},
    pages = {303--314},
    year = {1989}
}

@book{GoodfellowDL,
    author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
    title = {{D}eep Learning},
    publisher = {MIT Press},
    note = {\blueurl{https://www.deeplearningbook.org}},
    year = 2016,
    source = {actual paper}
}

@misc{He15,
    author = {He, Kayming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
    title = {Deep residual learning for image recognition},
    month = dec,
    year = 2015,
    howpublished = {arXiv:1512.03385}
}

@article{HornikNN89,
    author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halber},
    title = {Multilayer feedforward networks are universal approximators},
    journal = {Neural Netw.},
    volume = {2},
    number = 5,
    pages = {359--366},
    year = {1989}
}

@article{HornikNN91,
    author = {Hornik, Kurt},
    title = {Approximation capabilities of multilayer feedforward networks},
    journal = {Neural Netw.},
    volume = {4},
    number = {2},
    pages = {251--257},
    year = {1991}
}

@inproceedings{MaasICML13,
    author = {Maas, Andrew L. and Hannun, Awni Y. and Ng, Andrew, Y.},
    title = {Rectifier nonlinearities improve neural network acoustic models},
    booktitle = {Proceedings of the 30th International Conference on Machine Learning ({ICML})},
    series = {Proc. Mach. Learn. Res.},
    volume = 28,
    year = 2013,
    month = jun,
    address = {Atlanta, GA}
}

@inproceedings{MontufarNIPS14,
    author = {Mont\'ufar, Guido and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
    title = {On the number of linear regions of deep neural networks},
    booktitle = {Advances in Neural Information Processing Systems ({NIPS})},
    year = {2014},
    month = dec,
    address = {Montr\'eal, QC, Canada},
    pages = {2924--2932}
}

@misc{Pascanu13,
    author = {Pascanu, R. and Mont\'ufar, G. and Bengio, Y.},
    title = {On the number of response regions of deep feedforward networks with piecewise linear activations},
    note = {arXiv:1312.6098},
    month = dec,
    year = {2013}
}

@inbook{PressNR,
    author = {Press, William H. and Teukolsky, Saul A. and Vetterling, William T. and Flannery, Brian P.},
    title = {Numerical Recipes: The Art of Scientific Computing},
    edition = {3rd},
    year = 2007,
    chapter = 10,
    publisher = {Cambridge University Press},
    address = {New York, NY}
}

@inproceedings{RaghuICML17,
    author = {Raghu, Maithra and Poole, Ben and Kleinberg, Jon and Ganguli, Surya and Dickstein, Jascha S.},
    title = {On the expressive power of deep neural networks},
    booktitle = {Proceedings of the 34th International Conference on Machine Learning ({ICML})},
    series = {Proc. Mach. Learn. Res.},
    volume = 70,
    year = 2017,
    month = aug,
    address = {Sydney, Australia}
}

@techreport{RosenblattCornell57,
    author = {Rosenblatt, Frank},
    title = {The Perceptron--a perceiving and recognizing automaton},
    year = 1957,
    institution = {Cornell Aeronautical Laboratory},
    number = {85-460-1}
}

@article{SonodaACHA,
    author = {Sonoda, Sho and Murata, Noboru},
    title = {Neural network with unbounded activation functions is universal approximator},
    journal = {Appl. Comput. Harmon. Anal.},
    year = {2017},
    month = sep,
    volume = 43,
    number = 2,
    pages = {233-268}
}
